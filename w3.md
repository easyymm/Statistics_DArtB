# 통계학 3주차 정규과제

📌통계학 정규과제는 매주 정해진 분량의 『*데이터 분석가가 반드시 알아야 할 모든 것*』 을 읽고 학습하는 것입니다. 이번 주는 아래의 **Statistics_3rd_TIL**에 나열된 분량을 읽고 `학습 목표`에 맞게 공부하시면 됩니다.

아래의 문제를 풀어보며 학습 내용을 점검하세요. 문제를 해결하는 과정에서 개념을 스스로 정리하고, 필요한 경우 추가자료와 교재를 다시 참고하여 보완하는 것이 좋습니다.

2주차는 `2부-데이터 분석 준비하기`를 읽고 새롭게 배운 내용을 정리해주시면 됩니다.


## Statistics_3rd_TIL

### 2부. 데이터 분석 준비하기
### 08. 분석 프로젝트 준비 및 기획
### 09. 분석 환경 세팅하기



## Study Schedule

|주차 | 공부 범위     | 완료 여부 |
|----|----------------|----------|
|1주차| 1부 p.2~56     | ✅      |
|2주차| 1부 p.57~79    | ✅      | 
|3주차| 2부 p.82~120   | ✅      | 
|4주차| 2부 p.121~202  | 🍽️      | 
|5주차| 2부 p.203~254  | 🍽️      | 
|6주차| 3부 p.300~356  | 🍽️      | 
|7주차| 3부 p.357~615  | 🍽️      |  

<!-- 여기까진 그대로 둬 주세요-->

# 08. 분석 프로젝트 준비 및 기획

```
✅ 학습 목표 :
* 데이터 분석 프로세스를 설명할 수 있다.
* 비즈니스 문제를 정의할 때 주의할 점을 설명할 수 있다.
* 외부 데이터를 수집하는 방법에 대해 인식한다.
```
## 📍 8.1. 데이터 분석의 전체 프로세스

### ✏️ 8.1.1. 데이터 분석의 3단계

1️⃣ **설계 단계**
 
🔹 과제 정의 및 범위 설정  
🔹 인력 구성 및 PM 확보  
🔹 실무자 및 데이터 분석가 간 협의 체계 수립  

2️⃣ **분석 및 모델링 단계**

🔹 데이터 분석을 위한 데이터 마트 구축  
🔹 데이터 준비, 가공, **분석** 및 모델 도출-비즈니스적 적합성 분석 및 성능 평가 중요. KDD 분석 방법론, CRISP-DM 방법론, SAS사의 SEMMA 방법론 등  
🔹 모델 검증 및 실무, 경영진 협의  

3️⃣ **구축 및 활용 단계**

🔹 모델 적용 및 시스템 구축  
🔹 성과 평가 및 추가, 보완 프로젝트 검토  


### ✏️ 8.1.2. CRISP-DM 방법론

![Image](https://github.com/user-attachments/assets/36a6e353-13d2-4788-9d2f-1589196a4e82)

1️⃣ 비즈니스 이해  
  
🔹 현재 상황 평가    
🔹 데이터 마이닝 목표 결정  
🔹 프로젝트 계획 수립

2️⃣ 데이터 이해  
  
🔹 데이터 설명  
🔹 데이터 탐색  
🔹 데이터 품질 확인 

3️⃣ 데이터 준비  
  
🔹 데이터 선택  
🔹 데이터 정제  
🔹 필수 데이터 구성  
🔹 데이터 통합

4️⃣ 모델링
  
🔹 모델링 기법 선정  
🔹 테스트 디자인 생성  
🔹 모델 생성  
🔹 모델 평가

5️⃣ 평가
  
🔹 결과 평가  
🔹 프로세스 검토  
🔹 다음 단계 결정


6️⃣ 배포
  
🔹 배포 계획  
🔹 모니터링 및 유지 관리 계획  
🔹 최종 보고서 작성  
🔹 프로젝트 검토

### ✏️ 8.1.3. SAS SEMMA 방법론

1️⃣ Sampling 단계
  
🔹 전체 데이터에서 분석용 데이터 추출  
🔹 의미 있는 정보 추출을 위한 데이터 분할 및 병합  
🔹 표본 추출을 통해 대표성을 가진 분석용 데이터 생성  
🔹 분석 모델 생성을 위한 학습, 검증, 테스트 데이터셋 분할

2️⃣ Exploration 단계
  
🔹 통계치 확인, 그래프 생성 등을 통해 데이터 탐색  
🔹 상관분석, 클러스터링 등을 통해 변수 간 관계 파악  
🔹 분석 모델에 적합한 변수 선정  
🔹 데이터 현황을 파악하여 비즈니스 아이디어 도출 및 분석 방향 수정

3️⃣ Modification 단계
  
🔹 결측값 처리 및 최종 분석 변수 선정  
🔹 로그변환, 구간화(Binning) 등 데이터 가공  
🔹 주성분분석(PCA) 등을 통해 새로운 변수 생성 

4️⃣ Modeling 단계
  
🔹 다양한 데이터마이닝 기법 적용에 대한 적합성 검토  
🔹 비즈니스 목적에 맞는 분석 모델을 선정하여 분석 알고리즘 적용  
🔹 지도학습, 비지도학습, 강화학습 등 데이터 형태에 따라 알맞은 모델 선정  
🔹 분석 환경 인프라 성능과 모델 정확도를 고려한 모델 세부 옵션 설정

5️⃣ Assessment 단계
  
🔹 구축한 모델들의 예측력 등 성능을 비교, 분석, 평가  
🔹 비즈니스 상황에 맞는 적정 임계치(Cut off) 설정  
🔹 분석 모델 결과를 비즈니스 인사이트에 적용  
🔹 상황에 따라 추가적인 데이터 분석 수행

> 초반부-비즈니스 문제와 해결 방향 정의 데이터 탐색  
중반부-데이터 수집 및 가공, 필요에 따라 머신러닝 모델 사용  
후반부-분석 결과 검토 및 검증, 실제 환경에 적용, 모니터링 및 보완


## 📍 8.2. 비즈니스 문제 정의와 분석 목적 도출

▪️ 데이터 분석 프로젝트는 문제 정의 및 분석 목적을 명확히 정의하고 시작해야 한다. 이 과정이 조금이라도 잘못되면, 최종 인사이트 도출 및 솔루션 적용 단계에서 제대로 된 효과를 보기 힘들며, 후반부에 프로젝트의 목적성이 불분명해지기도 한다.

![Image](https://github.com/user-attachments/assets/421b19bb-a3ad-4292-9330-1c3d9fd6fbea)

▪️ **MECE**(Mutually Exclusive Collectively Exhaustive): 비즈니스 문제를 올바르게 정의하기 위한 논리적 접근법. 세부 정의들이 서로 겹치지 않고 전체를 합쳤을 때는 빠진 것 없이 완전한 전체를 이루는 것. 일반적으로 로직 트리를 활용하여 세부 항목을 정리한다. 핵심 문제에 대한 세부 문제나 과제 등이 MECE의 원칙에 따라 잘게 뻗어 나가도록 구성한다.

▪️ 비즈니스 문제는 명확하고 직관적인 한 문장으로 정리할 수 있어야 한다.  
예) 약정기간이 끝난 고객들이 타 통신사로 이탈한다. → 이탈 고객 예측 모델 개발  
약정기간이 끝난 고객들이 타 통신사로 이탈하여 회사의 수익이 감소하고 있다. → 이탈에 따른 손해 최소화하는 프로모션 최적화 모델 개발  

![Image](https://github.com/user-attachments/assets/9b734983-98b7-4d03-b89d-74324fef3c38)

▪️ 페이오프 매트릭스: 문제해결 우선순위 결정방식. 과제의 수익성과 실행 가능성 수준에 따라 2*2 4개의 분면에 과제 우선순위 표현. Ⅰ 우선 선정, Ⅱ 중 일부 선정, Ⅲ&Ⅳ의 과제는 제외.

## 📍 8.3. 분석 목적의 전환

▪️ 데이터 탐색 전까지 데이터에 숨겨져 있는 정보와 인사이트를 확인하기가 어려우므로, 분석 프로젝트의 방향이 언제들 바뀔 수 있다는 것을 염두에 두어야 한다.

▪️ 초기 데이터 탐색으로 도출할 수 있는 간단한 상관관계나 데이터 특성, 시각화를 적극 활용하여 실무자의 이해 및 관심을 유도해야 한다.

## 📍 8.4. 도메인 지식

▪️ 비즈니스 도메인을 이해하는 것은 곧 데이터 분석가의 전문 역량을 갖추는 것이다. 도메인 지식이 없는 상태에서는 분석 결과가 어떠한 의미를 가지는지, 어떻게 활용될 수 있는지 알기 힘들다. 직접 의미 있는 변수를 찾아내고 분석 방향을 설정하는 것은 도메인 지식이 충분하게 수반되었을 때 가능하다. 

▪️ 도메인 지식을 효과적으로 습득하기 위해서는,

🔹 비즈니스 도메인에 소속된 실무자와의 잦은 미팅, 적극적 질문과 자료 요청  
🔹 관련 논문들을 참고하여 해당 도메인에 대한 심도 있는 지식 습득-해당 도메인에서 특히 효과적인 데이터 마이닝 방법론이 있는 경우가 많고, 관련 논문 레퍼런스를 통해 분석에 사용된 모델에 대한 어느 정도의 보증 가능  
🔹 현장에 방문해 데이터가 만들어지는 과정을 직접 보고, 소비자 혹은 사용자의 입장이 되어 경험해보기

## 📍 8.5. 외부 데이터 수집과 크롤링

▪️ 외부데이터를 수집하고자 할 때에는, 분석 목적을 먼저 명확히 정의하고 이에 맞는 외부 데이터를 수집해야 한다.

![Image](https://github.com/user-attachments/assets/0ced8923-da35-4101-83c4-d1c4f1e9fa86)

▪️ 외부 데이터 수집 방법

1️⃣ 데이터 판매 전문 기업으로부터 구매 / MOU 등으로 데이터 공유: 비용이 많이 들거나 절차가 복잡하지만, 어느정도 정제된 고품질 데이터를 얻을 수 있음.  
2️⃣ 공공 오픈 데이터: 비용과 노력은 적게 드나, 원하는 형태로 가공하기 어렵고, 활용성이 높은 데이터를 얻을 확률이 낮음.    
3️⃣ 크롤링(스크래핑): 원하는 데이터를 실시간으로 자유롭게 수집 가능하나, 수집을 위한 프로그래밍이 필요하며 수정이 필요할 수 있고, 기업에서 활용시 법적인 부분을 고려해야 함.

▪️ 크롤링-웹사이트에 robots.txt 파일을 심어 두어 접속 주체에 따라 크롤링 허용 범위를 안내하고 있다. Open API를 통해 정리된 데이터를 제공받을 수도 있고, 직접 원하는 데이터를 수집하도록 코딩을 할 수도 있다. 파이썬에서는 BeautifulSoup이나 Selenium 라이브러리를 활용하여 크롤링을 할 수 있다.

🔹 User-agent: 대상 크롤러(모든 검색 봇, 구글 봇 등)
🔹 Allow: 허용하는 경로
🔹 Disallow: 허용하지 않는 경로

# 09. 분석 환경 세팅하기

```
✅ 학습 목표 :
* 데이터 분석의 전체적인 프로세스를 설명할 수 있다.
* 테이블 조인의 개념과 종류를 이해하고, 각 조인 방식의 차이를 구분하여 설명할 수 있다.
* ERD의 개념과 역할을 이해하고, 기본 구성 요소와 관계 유형을 설명할 수 있다.
```

## 📍 9.1. 어떤 데이터 분석 언어를 사용하는 것이 좋을까?

▪️ SAS: 제품형 데이터 분석 솔루션. 고급 및 예측 분석 솔루션 제공, 데이터 시각화 용이

▪️ R: 오픈소스 데이터 분석용 언어. 통계적 기능이 우수하며 데이터 시각화에 특화. 활발한 커뮤니티로 분석 패키지 공유, 질의 응답 가능. 파이썬에 비해 프로그래밍적 소양이 부족해도 사용하는 데 무리가 없다.

▪️ Python: C언어로 구현된 프로그래밍 언어로, 데이터 분석에 국한되지 않고 웹서비스, 응용 프로그램, IoT 등 다양한 분야에서 사용되는 유연한 언어. 커뮤니티가 활발하며 다양한 패키지 존재.

▪️ SQL: 관계형 데이터베이스 시스템에서 데이터를 관리 및 처리하기 위해 설계된 언어. 대화식 언어이기 때문에 명령문이 짧고 간결해 쉽게 익힐 수 있다. 

## 📍 9.2. 데이터 처리 프로세스 이해하기

![Image](https://github.com/user-attachments/assets/97113668-70d6-4d8c-bc71-4b9533f1fb92)

▪️ OLTP(On-Line Transaction Processing): 실시간으로 데이터를 트랜잭션 단위로 수집, 분류, 저장하는 시스템, 

▪️ DW(ODS): 수집된 데이터를 사용자 관점에서 주제별로 통합하여 쉽게 원하는 데이터를 빼낼 수 있도록 저장해 놓은 통합 데이터베이스.DW를 통해 OLTP를 보호하고 데이터 활용 효율을 높일 수 있다.

▪️ DM: 사용자의 목적에 맞도록 가공된 일부의 데이터가 저장되는 곳. 부서나 사용자 집단의 필요에 맞도록 가공된 개별 데이터 저장소. 접근성과 데이터 분석의 효율성을 높이고, DW의 시스템 부하를 감소시킴.

🔹 ETL(Extract, Transform, Load): 저장된 데이터를 사용자가 요구하는 포맷으로 변형하여 이동시키는 작업 과정. 

## 📍 9.3. 분산데이터 처리

▪️ scale-up: 빅데이터를 처리하기 위해 하나의 컴퓨터의 용량을 늘리고 더 빠른 프로세서를 탑재하는 것. 컴퓨터 성능이 아무리 좋아진다 하더라도 결국 하나의 컴퓨터가 모든 데이터를 처리해야 하므로, 데이터의 크기가 커지면 속도가 급격히 느려짐.

▪️ scale-out: 분산데이터 처리처럼 여러 대의 컴퓨터를 병렬적으로 연결. 여러 대의 컴퓨터가 함께 연산하므로 효율이 훨씬 높음.

### ✏️ 9.3.1. HDFS

▪️ HDFS

🔹 슬레이브 노드: 데이터 저장, 계산  
🔹 마스터 노드: 대량의 데이터를 HDFS에 저장, 맵리듀스 방식으로 데이터 병렬 처리  
🔹 클라이언트 머신: 맵리듀스 작업을 통해 산출된 결과를 사용자에게 보여줌  

▪️ 맵리듀스

🔹 HDFS에 저장된 데이터를 효과적으로 처리하는 방법.  
🔹 맵 단계: 흩어져 있는 데이터를 관련된 데이터끼리 묶어 임시의 집합을 만드는 과정.  
🔹 리듀스: 필터링과 정렬을 거쳐 데이터를 뽑아냄.  
🔹 key-value 쌍으로 데이터를 처리함.  

예) 단어 수 세기  
1️⃣ 분할: 입력된 데이터를 고정된 크기의 조각으로 분할  
2️⃣ 매핑: 분할된 데이터를 key-value 형태로 묶어 단어 개수 계산  
3️⃣ 셔플링: 매핑 단계의 counting 결과를 정렬 및 병합  
4️⃣ 리듀싱: 각 결과를 취합 및 계산하여 최종 결괏값 산출  

▪️ 하둡

🔹 하둡 1.0(HDFS, 맵리듀스), 하둡 2.0(+YARN)  
🔹 기본적 리소스 관리 시스템 JobTracker: 전체 클러스터의 리소스 관리, 수행 중인 Job들의 진행상황·에러 관리, 완료된 Job들의 로그 저장 및 확인  
🔹 YARN: 각 클러스터마다 애플리케이션 마스터 존재, 여러 Job들이 성공적으로 실행될 수 있도록 리소스 관리, 스케줄링, 모니터링 기능 등을 제공. 필요 자원은 리소스 매니저를 통해 할당받음. Job에 대한 로그 이력 관리는 타임라인 서버로 함. 노드 매니저-모든 노드에서 실행되어 각각의 할당된 태스크 실행, 진행 상황 관리

▪️ 분산 시스템 구조

![Image](https://github.com/user-attachments/assets/489c536d-268e-4791-b255-24060a4fecf6)

### ✏️ 9.3.2. 아파치 스파크

▪️ HDFS와 스파크

![Image](https://github.com/user-attachments/assets/94a96f3d-cdf6-4d20-aab1-6456a176e22d)

🔹 아파치 스파크를 통해 데이터 분석 환경을 구축하는 기업이 많아짐.  
🔹 스파크는 분산 데이터 처리를 하는 하나의 시스템으로, 기존 하둡의 맵리듀스 방식보다 성능이 개선되어 빠름. 머신러닐 같은 반복형이나 대화형 태스크에서 성능 뛰어남.  

▪️ 스파크의 특징

🔹 데이터 과학자 및 분석가들이 효율적으로 일할 수 있도록 특화된 환경을 가짐. 데이터 마이닝과 같은 온라인 분석 처리(OLAP) 작업에는 특화됨, 대량의 온라인 트랜잭션 처리(OLTP)와 같은 대량의 원자성 트랜잭션 처리에는 적합하지 않음.

▪️ 제플린

🔹 스파크 환경에셔의 웹 기반 노트북이자 시각화 툴.  
🔹 순수한 파이썬 언어로도 데이터 가공과 모델링 가능, 스파츠의 병렬처리를 효과적으로 활용하기 위해 스파크 전용 파이썬 코드인 파이스파크(PySpark), 스파크 환경의 SQL인 Spark SQL 등을 사용할 수 있음.

# 📍 9.4. 테이블 조인과 정의서 그리고 ERD

### ✏️ 9.4.1. 테이블 조인

▪️ LEFT JOIN, RIGHT JOIN

🔹 하나의 테이블을 기준으로 다른 테이블에서 겹치는 부분 결합  
🔹 일치하는 키 값이 없는 행은 조인하는 테이블의 값이 결측값으로 나타남

▪️ INNER JOIN

🔹 두 테이블 간 겹치는 부분의 행만 가져옴

▪️ FULL JOIN

🔹 모든 행을 살림

▪️ CROSS JOIN

🔹 두 개의 테이블에 있는 모든 행을 조합

🔹 주로 머신러닝에 사용되는 데이터셋 생성 시 사용

### ✏️ 9.4.2. 데이터 단어사전

▪️ 데이터 단어사전: 각 컬럼과 테이블 이름을 정할 때 체계를 약속한 일종의 사전. 축약된 단어 형태로 정하며, 기업/조직마다 사용하는 단어 사전 법칙에는 차이가 있으나 대부분 형태가 유사하다.

▪️ 메타데이터 관리 시스템: 데이터가 어디에 어떻게 저장되어 있는지, 데이터를 어떻게 사용할 것인지 이해할 수 있도록 데이터에 대한 정보를 관리하는 시스템.

### ✏️ 9.4.3. 테이블 정의서

▪️ 메타데이터 관리 시스템을 간소화한 버전. 각 DW, DM 등에 적재된 테이블과 컬럼의 한글과 영문명, 데이터 속성, 간단한 설명 등이 정리된 표. 보통 엑셀 파일로 만듦.

▪️ DB 환경에 익숙하지 않은 상태에서는 사용이 불편할 수 있어, 데이터 환경을 처음 이해하기 위해서는 ERD를 보아야 함.

### ✏️ 9.4.4. ERD

▪️ 각 테이블의 구성 정보와 테이블 간 관계를 도식으로 표현한 그림 형태로 구성됨. 각 테이블(엔티티)이 어떤 테이블과 어떤 키로 연결되어 있는지 직관적으로 확인할 수 있음.

▪️ 물리 ERD: DB를 효율적이고 결점 없이 구현하는 것이 목표  
▪️ 논리 ERD: 데이터 사용자 입장에서 테이블 간 매핑에 오류가 없으며 데이터의 정규화가 이루어진 ERD의 개념

▪️ 테이블 간 연결을 해주는 키 컬럼과 연결 관계를 의미하는 식별자가 핵심.

🔹 기본키: 테이블에 적재된 각각의 데이터를 유일하게 구분하는 키. 중복, 결측값 없어야 함.  
🔹 외래키: 각 테이블 간 연결을 만들기 위해 테이블에서 다른 테이블의 참조되는 기본 키. 중복, 결측값 있을 수 있음. 외래키가 정의된 테이블은 자식테이블, 참조되는 테이블은 부모테이블.

▪️ 테이블 간 관계를 정확히 파악하고 데이터를 다루어야 한다.(1:1, 1:N, N:N 등)

<br>
<br>

# 확인 문제

## 문제 1.

> **🧚 아래의 테이블을 조인한 결과를 출력하였습니다. 어떤 조인 방식을 사용했는지 맞춰보세요.**

> 사용한 테이블은 다음과 같습니다.

![TABLE1](https://github.com/ejejbb/Template/raw/main/File/2.6.PNG)|![TABLE2](https://github.com/ejejbb/Template/raw/main/File/2.7.PNG)
---|---|

> 보기: INNER, LEFT, RIGHT 조인

<!-- 테이블 조인의 종류를 이해하였는지 확인하기 위한 문제입니다. 각 테이블이 어떤 조인 방식을 이용하였을지 고민해보고 각 테이블 아래에 답을 작성해주세요.-->

### 1-1. 
![TABLE](https://github.com/ejejbb/Template/raw/main/File/2-1.PNG)
```
LEFT JOIN
```

### 1-2. 
![TABLE](https://github.com/ejejbb/Template/raw/main/File/2-3.PNG)
```
INNER JOIN
```

### 1-3. 
![TABLE](https://github.com/ejejbb/Template/raw/main/File/2-2.PNG)
```
RIGHT JOIN
```

### 🎉 수고하셨습니다.