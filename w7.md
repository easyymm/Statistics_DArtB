# 통계학 7주차 정규과제

📌통계학 정규과제는 매주 정해진 분량의 『*데이터 분석가가 반드시 알아야 할 모든 것*』 을 읽고 학습하는 것입니다. 이번 주는 아래의 **Statistics_7th_TIL**에 나열된 분량을 읽고 `학습 목표`에 맞게 공부하시면 됩니다.

아래의 문제를 풀어보며 학습 내용을 점검하세요. 문제를 해결하는 과정에서 개념을 스스로 정리하고, 필요한 경우 추가자료와 교재를 다시 참고하여 보완하는 것이 좋습니다.

7주차는 `3부. 데이터 분석하기`를 읽고 새롭게 배운 내용을 정리해주시면 됩니다.


## Statistics_7th_TIL

### 3부. 데이터 분석하기
### 13.머신러닝 분석 방법론
### 14.모델 평가



## Study Schedule

|주차 | 공부 범위     | 완료 여부 |
|----|----------------|----------|
|1주차| 1부 p.2~56     | ✅      |
|2주차| 1부 p.57~79    | ✅      | 
|3주차| 2부 p.82~120   | ✅      | 
|4주차| 2부 p.121~202  | ✅      | 
|5주차| 2부 p.203~254  | ✅      | 
|6주차| 3부 p.300~356  | ✅      | 
|7주차| 3부 p.357~615  | ✅      | 

<!-- 여기까진 그대로 둬 주세요-->

# 13.머신러닝 분석 방법론

```
✅ 학습 목표 :
* 선형 회귀와 다항 회귀를 비교하고, 데이터를 활용하여 적절한 회귀 모델을 구축할 수 있다. 
* 로지스틱 회귀 분석의 개념과 오즈(Odds)의 의미를 설명하고, 분류 문제에 적용할 수 있다.
* k-means 알고리즘의 원리를 설명하고, 적절한 군집 개수를 결정하여 데이터를 군집화할 수 있다.
```

## 📍 13.1. 선형 회귀분석과 Elastic Net(예측모델)
<!-- 새롭게 배운 내용을 자유롭게 정리해주세요.-->
<!-- `13.1.3. Ridge와 Lasso 그리고 Elastic Net` 부분은 제외하고 학습하셔도 무방합니다.-->

### ✏️ 13.1.1. 회귀분석의 기원과 원리

▪️ 회귀 분석: 종속변수 Y의 값에 영향을 주는 독립변수 X들의 조건을 고려하여 구한 평균값

▪️ **최소제곱추정법**: 예측치와 관측치들 간의 수직 거리(오차)의 제곱합을 최소로 하는 직선을 구하는 방법. 잔차제곱합(RSS)를 최소화하는 알파와 베타 값을 찾는 것을 의미.

▪️ 회귀분석 시에는 다음의 기본 조건이 충족되어야 함

🔹 다중공선성: 독립변수 간 상관관계가 없어야 함. 상관분석, VIF값 확인 등으로 확인, 차원축소나 변수 가공으로 방지.  
🔹 잔차의 정규성: 잔차의 히스토그램으로 정규분포 모양 확인, Q=Q Plot, 샤피로-윌크 검정, 앤더슨-달링 검정  
🔹 잔차의 등분산성  
🔹 독립성  
🔹 선형성  

### ✏️ 13.1.2. 다항 회귀

▪️ **다항회귀**: 독립변수와 종속변수의 관계가 곡선형 관계일 때 변수에 각 특성의 제곱을 추가하여 회귀선을 곡선형으로 변환하는 모델. 

▪️ 회귀분석의 기본 가설

🔹 귀무가설: 모든 회귀계수가 0이다.  
🔹 대립가설: 적어도 하나의 변수는 회귀계수가 0이 아니다.

▪️ **T Value**

🔹 노이즈 대비 시그널의 강도, 독립변수와 종속변수 간 선형관계가 얼마나 강한지를 나타내므로 값이 커야 함.  
🔹 계숫값을 표준오차로 나누어 구함  
🔹 절댓값의 크기로 변수의 유의성 확인: 관측치 100개 기준, T Value 절댓값이 1.98을 넘으면 0.05 유의수준 임계치를 넘어서므로 유의하다고 판단

▪️ **P Value**

🔹 T Value와 관측치 수에 의해 결정되는 값.  
🔹 일반적으로 0.05 이해의 값인 경우에 95% 귀무가설 기각역에 포함하므로 해당 변수가 유의하다고 판단  
🔹 경우에 따라 0.1이나 0.01의 기준을 사용하기도 함

▪️ 변수 조합 선택 알고리즘

🔹 **전진 선택법**: 절편만 있는 모델에서 유의미한 독립변수 순으로 변수 차례대로 추가. 알고리즘이 단순해 빠르고, 한번 선택된 변수는 다시 제거되지 않음  
🔹 **후진 제거법**: 모든 독립변수가 포함된 상태에서 유의미하지 않은 순으로 하나씩 제거. 한번 제거된 변수는 다시 추가되지 않음. 시간이 다소 오래 걸리나, 전진선택법보다는 안전한 방법.  
🔹 **단계적 선택법**: 전진선택법과 같이 변수를 하나씩 추가하다가, 선택된 변수가 3개 이상이면 변수 추가와 제거를 번갈아가며 수행. 선택된 독립변수 모델의 잔차를 구하여 선택되지 않은 나머지 변수와 잔차의 상관도를 구해 최적의 변수 조합을 찾아냄, 오래 걸림.


## 📍 13.2. 로지스틱 회귀분석 (분류모델)
<!-- 새롭게 배운 내용을 자유롭게 정리해주세요.-->

▪️ 로지스틱 회귀분석

🔹 종속변수가 질적 척도, 카테고리 분류 모델.

▪️ **이항 로지스틱 회귀 분석**

![Image](https://github.com/user-attachments/assets/5cdb7a82-aa43-43fb-bd3a-b38b3d95c157)

▪️ 로짓 변환

🔹 0과 1 사이의 S자 곡선의 형태를 갖도록 변환

▪️ 오즈

🔹 사건이 발생할 가능성이 발생하지 않을 가능성보다 어느 정도 큰지를 나타내는 값.

![Image](https://github.com/user-attachments/assets/87d1f287-40c0-4f12-b85b-c6acaf9dd0bb)

![Image](https://github.com/user-attachments/assets/edceff7e-d407-43df-94a0-fb85bdaee74b)


🔹 발생 확률이 1에 가까워질수록 오즈 값은 기하급수적으로 커지고 최솟값은 0이 되므로, 오즈 값에 로그를 취해 확률의 범위를 표현 -> 로짓 변환하여 0에서 1 사이로 치환해줌. = 시그모이드 함수. 

![Image](https://github.com/user-attachments/assets/6ddbdf78-c995-4beb-9563-5db4920e7161)


▪️ **다항 로지스틱 회귀 분석**

🔹 범주마다 이항 로지스틱을 시행하여 확률을 구함.   
🔹 하나의 범주를 기준으로 잡고, 나머지 다른 변수들과 비교하여 식을 만듦.

▪️ 결과 해석

🔹 각 변수에 대한 계숫값과 유의도 확인 가능  
🔹 오즈비   
🔹 R^2값  
🔹 테스트세트에서 실제로 얼마나 올바르게 분류했는가를 보고 성능을 평가하는 것이 좋음: ROC Curve, Confusion matrix

## 📍 13.8. k-means 클러스터링(군집모델)
<!-- 새롭게 배운 내용을 자유롭게 정리해주세요.-->

▪️ k: 분류할 군집의 수, means: 각 군집의 중심.

▪️ 중심점과 군집 내 관측치 간의 거리를 비용함수로 하여, 이 함수 값이 최소화되도록 중심점과 군집을 반복적으로 재정의해줌.

1️⃣ k개의 중심점을 임의의 데이터 공간에 선정  
2️⃣ 각 중심점과 관측치들 간의 유클리드 거리를 계산  
3️⃣ 각 중심점과 거리가 가까운 관측치들을 해당 군집으로 할당  
4️⃣ 할당된 군집의 관측치들과 해당 중심점과의 유클리드 거리 계산  
5️⃣ 중심점을 군집의 중앙으로 이동(군집의 관측치들 간 거리 최소 지점)  
6️⃣ 중심점이 더이상 이동하지 않을 때까지 2~5단계 반복

▪️ 지역 최솟값(Local Minimum) 문제

![Image](https://github.com/user-attachments/assets/9863ed3e-d08b-42af-b3a7-b168c3c1246f)

🔹 거리합이 최소화되는 전역 최솟값(Global minimum)을 찾기 전 지역 최솟값에서 알고리즘이 종료되는 것  
🔹 초기 중심점 선정 방법을 다양하게 하여 최적 모델 선정  
🔹 랜덤 방식, 중심점들을 가능한 한 서로 멀리 떨어져서 설정하는 방법, 중심점이 밀집되지 않도록 하는 k-means++ 등

▪️ 적절한 k의 수

🔹 도메인 지식을 통한 개수 선정  
🔹 엘보우 기법: 군집 내 중심점과 관측치 간 거리 합이 급감하는 구간의 k 개수를 선정하는 방법. 거리합이 급감했다는 것은 유사한 속성의 관측치들끼리 잘 묶였음을 뜻함, k가 더 증가해도 거리합이 별로 줄어들지 않을 때는 k를 더 증가시킬 필요가 없다는 것을 의미함.   
🔹 실루엣 계수: 군집 안의 관측치들이 다른 군집과 비교하여 얼마나 비슷한지를 나타내는 수치. 동일 군집 안에 있는 관측치들 간의 평균 거리와 가장 가까운 다른 군집과의 평균 거리를 구해 실루엣 계수를 구함.

▪️ DBSCAN

🔹 관측치들의 밀도를 통해 자동으로 적절한 군집의 수를 찾는 방법  
🔹 k 수 지정이 필요 없음  
🔹 밀도 측정을 위한 두 가지 기준: 거리 기준(epsilon, 기준 관측치로부터 이웃한 관측치인지 구별), 관측치 수(minPts, 특정 거리 안에 몇 개 이상의 관측치가 있어야 하나의 군집으로 판단할 것인가)  
🔹 U자형과 같이 오목한 데이터나 H와 같은 모양을 띄는 데이터 분포를 효과적으로 군집화할 수 있음  
🔹 군집이 명확하지 않은 이상치를 잘 분류  
🔹 연산량이 많아 변수를 적절히 설정해 주어야 하며, 데이터 특성을 모를 경우에는 적절한 파라미터 값을 설정하는 것이 어려움

▪️ 군집 특징 정의

🔹 사용한 독립변수들이 다른 군집들에 비해 어떠한 특성을 가지고 있는지를 확인하여 각 군집을 명확하게 정의하고, 그에 맞는 비즈니스 전략을 수립해야 함.  

▪️ 군집 간 수치적 차이를 보다 확실하게 파악하기 위해서는 t-test나 ANOVA 등으로 군집 간 평균값 차이가 우연한 차이인지, 통계적으로 유의미한 차이인지 검증하는 절차를 거치는 것이 좋음

# 14. 모델 평가

```
✅ 학습 목표 :
* 유의확률(p-value)을 해석할 때 주의할 점을 설명할 수 있다.
* 분석가가 올바른 주관적 판단을 위한 필수 요소를 식별할 수 있다.
```

## 📍 14.3. 회귀성능 평가지표
<!-- 새롭게 배운 내용을 자유롭게 정리해주세요.-->

### ✏️ 14.3.1. R-Square와 Adjusted R-Square

▪️ **결정계수(R-Square)**

🔹 실젯값과 예측값의 차이인 오차워, 실젯값과 실젯값 평균의 차이인 편차와 관련

![Image](https://github.com/user-attachments/assets/c68dc1f3-190b-4691-bbd5-eb90cc95c86e)

![Image](https://github.com/user-attachments/assets/8e5bfc1b-13c3-438a-9abf-1e82890cda0d)

🔹 **SSR**: 회귀식의 추정값과 전체 실젯값 평균과의 편차 제곱합  
🔹 **SSE**: 회귀식의 추정값과 실젯값 편차 제곱의 합  
🔹 **SST**: 실젯겞과 전체 실젯값 평균과의 편차 제곱합  
🔹 R-Square 값은 SSR 값이 클수록, SST 값이 작을수록 커지게 됨. 

▪️ **Adjusted R-Square**

![Image](https://github.com/user-attachments/assets/8b262dcd-c496-4e03-9fb6-3c3066b9a615)

🔹 독립변수의 개수가 많아질수록 값이 커지는 문제를 보정한 기준

### ✏️ 14.3.2. RMSE(Root Mean Square Error)

![Image](https://github.com/user-attachments/assets/d88fa044-bdff-4bae-add3-744b28a05ee4)

▪️ 

🔹 예측값과 실젯값의 차이가 평균적으로 어느 정도인지 측정할 수 있어 직관적으로 모델의 정확도를 가늠할 수 있음  
🔹 예측값의 스케일에 영향을 받으므로, 표본 데이터가 다르면 RMSE 절대치로 비교해서는 안 됨.

### ✏️ 14.3.3. MAE(Mean Absolute Error)

![Image](https://github.com/user-attachments/assets/46121f9b-5020-4436-bb19-faddef51baf0)

🔹 실젯값과 예측값의 차이 절댓값 합을 n으로 나눈 값, 직관적으로 예측값 차이를 비교할 수 있음

### ✏️ 14.3.4. MAPE(Mean Absolute Percentage Error)

![Image](https://github.com/user-attachments/assets/c7c319b2-be3a-4829-900b-d188dd695165)

🔹 MAE를 퍼센트로 변환한 것. 스케일에 관계없이 절대적인 차이를 비교할 수 있으므로 다른 데이터가 들어간 모델 간 성능 비교에 유용.  
🔹 실젯값이 0인 경우 0으로 나눌 수 없기 때문에 MAPE를 구할 수 없음-> 실젯값에 0이 많은 데이터는 MAPE 평가 기준을 사용하는 것이 적합하지 않음  
🔹 실젯값이 양수인 경우, 실젯값보다 작은 값으로 예측하는 경우 MAPE의 최댓값이 최대 100%까지만 커질 수 있음. 실젯값보다 크게 예측하는 경우 MAPE값이 한계가 없기 때문에 MAPE 기준으로 모델을 학습하면 실젯값보다 작은 값으로 예측하도록 편향될 수 있음  
🔹 실젯값이 0과 가까운 매우 작은 값인 경우에 MAPE가 과도하게 높아지는 경우가 발생할 수 있음

### ✏️ 14.3.5. RMSLE(Root Mean Square Logarithmic Error)

![Image](https://github.com/user-attachments/assets/f85f70b0-eb66-4403-a370-cb7fde49eedb)

🔹 로그를 취해 상대적 비율을 비교할 수 있어, RMSE보다 오차 이상치에 덜 민감    
🔹 절댓값 오차의 스케일 차이가 나더라도 오차 비율이 같으면 거의 동일한 값 산출 

### ✏️ 14.3.6. AIC와 BIC

▪️ AIC

🔹 최대 우도에 독립변수가 얼마나 많은가에 따른 페널티를 반영하여 계산하는 모델 평가 척도. 모델의 정확도와 함께 독립변수가 많고 적음까지 따져서 우수한 모델을 선택할 수 있도록 하는 평가 방법.  
🔹 값이 작을수록 좋은 모델. 우도가 높을수록, 변수가 적을수록 값이 작아짐. 변수가 늘어날수록 모델의 편의(bias)는 감소하지만 분산(variance)은 증가함.   
🔹 적정한 변수의 개수를 찾는 데에 유용한 모델 평가 방법
🔹 관측치가 적은 경우 관측치 수를 반영하여 보정된 AICc 방식 사용 가능.

▪️ BIC

🔹 관측치의 개수에 자연로그를 취한 값을 독립변수의 개수와 곱함  
🔹 변수의 개수를 줄이는 것이 중요할 때는 BIC로 평가하는 것이 좋음


## 📍 14.6. 유의확률의 함정
<!-- 새롭게 배운 내용을 자유롭게 정리해주세요.-->

▪️ p값은 표본의 크기가 커지면 점점 0으로 수렴하는 특성이 있음. 표본의 크기가 커지면 표본 오차가 작아지고, 결과적으로 p값도 작아지게 됨.

▪️ p값의 기준인 0.05는 통상적으로 쓰이는 임의적인 기준이기 때문에 0.05 미만으로 나왔다고 해서 통계적 유의성이 확실히 있다고 단언할 수 없음. 기준값을 낮추거나 p값과 함께 효과 크기를 고려한 수치 기준 및 베이지안 통계치를 추가적으로 고려하여 귀무가설 기각 여부를 결정해야 한다는 의견이 있음.

▪️ 미국 통계학회(ASA)의 통계적 중요성 및 p값에 대한 성명서(2016)  

🔹 p값은 데이터가 통계 모델과 얼마나 호환되지 않는지를 나타낼 수 있다. 
🔹 p값은 연구 가설이 참일 확률 또는 데이터가 우연만으로 생성됐을 확률을 측정하지 않는다.  
🔹 과학적 결론과 사업 또는 정책 결정은 p값이 특정 임곗값을 통과하는지에 기초해서는 안 된다.  
🔹 적절한 추론에는 완전한 보고와 투명성이 필요하다.  
🔹 p값 또는 통계적 유의성은 효과의 크기나 결과의 중요성을 측정하지 않는다.  
🔹 p값은 그 자체로 모델이나 가설에 관한 증거에 대한 훌륭한 척도가 되지 않는다.  

## 📍 14.7. 분석가의 주관적 판단과 스토리텔링
<!-- 새롭게 배운 내용을 자유롭게 정리해주세요.-->

▪️ 분석가의 올바른 주관적 판단을 위해 필요한 것

🔹 해당 분야의 도메인 지식  
🔹 통계적 지식 기반 EDA와 전처리를 성실히 수행     
🔹 적극적인 커뮤니케이션과 검증 과정

▪️ 스토리텔링이 중요함

🔹 '배경-문제(위기)-극복-변화'의 흐름으로 구성  
🔹 해당 도메인이 익숙하지 않은 사람들에게는 배경을 간략히 소개  
🔹 불필요한 수식이나 표 최소화, 전문용어 최대한 배제


<br>
<br>

# 확인 문제

## **문제 1. 선형 회귀**

> **🧚 칼 피어슨의 아버지와 아들의 키 연구 결과를 바탕으로, 다음 선형 회귀식을 해석하세요.**  
> 칼 피어슨(Karl Pearson)은 아버지(X)와 아들(Y)의 키를 조사한 결과를 바탕으로 아래와 같은 선형 회귀식을 도출하였습니다. 아래의 선형 회귀식을 보고 기울기의 의미를 설명하세요. 
>  
> **ŷ = 33.73 + 0.516X**  
>   
> - **X**: 아버지의 키 (cm)  
> - **ŷ**: 아들의 예상 키 (cm)  

```
아버지의 키가 1cm 증가할 때, 아들의 예상 키는 평균적으로 0.516cm 증가한다.
```
---

## **문제 2. 로지스틱 회귀**  

> **🧚 다트비에서는 학생의 학업 성취도를 예측하기 위해 다항 로지스틱 회귀 분석을 수행하였습니다. 학업 성취도(Y)는 ‘낮음’, ‘보통’, ‘높음’ 3가지 범주로 구분되며, 독립 변수는 주당 공부 시간(Study Hours)과 출석률(Attendance Rate)입니다. 단, 기준범주는 '낮음' 입니다.**   

| 변수 | Odds Ratio Estimates | 95% Wald Confidence Limits |  
|------|----------------------|--------------------------|  
| Study Hours | **2.34** | (1.89, 2.88) |  
| Attendance Rate | **3.87** | (2.92, 5.13) |  

> 🔍 Q1. Odds Ratio Estimates(오즈비, OR)의 의미를 해석하세요.

<!--변수 Study Hours의 오즈비 값이 2.34라는 것과 Attendance Rate의 오즈비 값이 3.87이라는 것이 각각 무엇을 의미하는지 구체적으로 생각해보세요.-->

```
변수가 1단위 증가할 때 학업 성취도가 기준범주(낮음)에 비해 다른 범주(보통, 높음)에 속할 오즈가 해당 오즈비만큼 증가한다.
```

> 🔍 Q2. 95% Wald Confidence Limits의 의미를 설명하세요.
<!--각 변수의 신뢰구간에 제시된 수치가 의미하는 바를 생각해보세요.-->

```
모집단의 오즈비 모수가 해당 구간에 포함될 확률이 95%임을 의미하는 신뢰구간.
두 변수 모두 1을 포함하지 않으므로 유의미한 영향력이 있다고 판단.
```

> 🔍 Q3. 이 분석을 기반으로 학업 성취도를 향상시키기 위한 전략을 제안하세요.
<!--Study Hours와 Attendance Rate 중 어느 변수가 학업 성취도에 더 큰 영향을 미치는지를 고려하여, 학업 성취도를 향상시키기 위한 효과적인 전략을 구체적으로 제시해보세요.-->

```
오즈비의 크기를 비교해보면 학업 성취도에는 공부시간보다 출석률이 더 큰 영향을 미친다는 결론이 나오므로,
출석률을 개선하는 방향으로 전략을 제안하는 것이 우선이다.

- 출석 관리 체계(출석 시스템 및 기준)를 명확히 하고,
- 출석률에 따른 이점(포인트 제도 등)을 통해 자발적인 출석을 이끌어 내는 등의 전략이 필요하다.

다만 로지스틱 회귀는 인과 관계를 입증하지는 않으므로, 
출석률이 높으면 성취도가 높은 것인지, 성취도가 높은 학생들이 출석을 더 잘하는 것인지는 단정할 수 없다. 
따라서 전략을 적용하기 전 추가 분석이 필요할 것이다.
```

---


## **문제 3. k-means 클러스터링**

> **🧚 선교는 고객을 유사한 그룹으로 분류하기 위해 k-means 클러스터링을 적용했습니다. 초기에는 3개의 군집으로 설정했지만, 결과가 만족스럽지 않았습니다. 선교가 최적의 군집 수를 찾기 위해 사용할 수 있는 방법을 한 가지 이상 제시하고 설명하세요.**

```
엘보우 메서드: 클러스터 수 k에 따라 군집 내 거리합 감소 추이를 관찰해, 급격한 감소가 완만해지는 지점의 k값을 최적의 군집 수로 선택

실루엣계수: 관측치가 자신이 속한 군집 내에서는 얼마나 밀접하고, 다른 군집과는 얼마나 분리되어 있는지를 수치로 나타냄
```

### 🎉 수고하셨습니다.